{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/W952944798/info/blob/main/%E5%A4%8D%E5%88%B6%E2%80%9CDiscord_chat_bot_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7s4WsVY4U3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbb673b-815c-4bb5-e7e4-72b38f4dbf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting curl_cffi\n",
            "  Downloading curl_cffi-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from curl_cffi) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from curl_cffi) (2024.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12.0->curl_cffi) (2.21)\n",
            "Installing collected packages: curl_cffi\n",
            "Successfully installed curl_cffi-0.6.2\n",
            "数据已经保存到CSV文件：combined_data.csv\n"
          ]
        }
      ],
      "source": [
        "!pip install curl_cffi\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from curl_cffi import requests\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class DataAggregator:\n",
        "    def __init__(self, url, cookies):\n",
        "        self.url = url\n",
        "        self.cookies = cookies\n",
        "        self.spawn_intervals = {\n",
        "            ('Rare', 'Small'): 72,\n",
        "            ('Rare', 'Medium'): 66,\n",
        "            ('Rare', 'Large'): 60,\n",
        "            ('Epic', 'Small'): 66,\n",
        "            ('Epic', 'Medium'): 60,\n",
        "            ('Epic', 'Large'): 54,\n",
        "            ('Legendary', 'Small'): 60,\n",
        "            ('Legendary', 'Medium'): 54,\n",
        "            ('Legendary', 'Large'): 48,\n",
        "            ('Mythic', 'Small'): 54,\n",
        "            ('Mythic', 'Medium'): 48,\n",
        "            ('Mythic', 'Large'): 42,\n",
        "            ('Exalted', 'Small'): 48,\n",
        "            ('Exalted', 'Medium'): 42,\n",
        "            ('Exalted', 'Large'): 36\n",
        "        }\n",
        "        # 提取所需信息\n",
        "        self.extracted_info = []\n",
        "        self.aggregated_data = defaultdict(lambda: {'count': 0, 'issuedIds': []})\n",
        "\n",
        "    def fetch_data(self):\n",
        "        response = requests.get(self.url, impersonate='chrome110', cookies=self.cookies)\n",
        "        if response.status_code == 200:\n",
        "            # 解析返回的JSON数据\n",
        "            data = response.json()\n",
        "            for item in data[\"items\"]:\n",
        "                issued_id = item[\"issuedId\"]\n",
        "                name = item[\"metadata\"][\"name\"]\n",
        "                last_cracked_hour_glass_drop_time = None\n",
        "                for attribute in item[\"extra\"][\"attributes\"]:\n",
        "                    if attribute[\"name\"] == \"LastCrackedHourGlassDropTime\":\n",
        "                        last_cracked_hour_glass_drop_time = attribute[\"value\"]\n",
        "                        break\n",
        "\n",
        "                self.extracted_info.append({\n",
        "                    \"issuedId\": issued_id,\n",
        "                    \"name\": name,\n",
        "                    \"LastCrackedHourGlassDropTime\": last_cracked_hour_glass_drop_time\n",
        "                })\n",
        "            return self.extracted_info\n",
        "\n",
        "    def process_data(self, data):\n",
        "        for item in data:\n",
        "            # 从name属性提取rarity和size\n",
        "            name_parts = item['name'].split()  # ['Epic', 'Medium', 'SPACE']\n",
        "            rarity = name_parts[0]  # 'Epic'\n",
        "            size = name_parts[1]  # 'Medium'\n",
        "            issued_id = item[\"issuedId\"]\n",
        "\n",
        "            # 提取和计算下一次生成时间\n",
        "            next_spawn_str = self.calculate_next_spawn(rarity, size, item)\n",
        "            if next_spawn_str != \"Invalid rarity or size\":\n",
        "                key = (rarity, size, next_spawn_str)\n",
        "                self.aggregated_data[key]['count'] += 1\n",
        "                self.aggregated_data[key]['issuedIds'].append(issued_id)\n",
        "\n",
        "    def calculate_next_spawn(self, rarity, size, item):\n",
        "        dt_parts = item['LastCrackedHourGlassDropTime'].split('T')\n",
        "        date_parts = dt_parts[0].split('-')  # ['2024', '03', '04']\n",
        "        time_parts = dt_parts[1].split(':')  # ['07', '07', '59.545Z']\n",
        "        year = int(date_parts[0])  # 2024\n",
        "        month = int(date_parts[1])  # 03\n",
        "        day = int(date_parts[2])  # 04\n",
        "        hour = int(time_parts[0])  # 07\n",
        "        minute = int(time_parts[1])  # 07\n",
        "\n",
        "        last_drop_time = datetime(year, month, day, hour, minute, tzinfo=pytz.utc)\n",
        "        interval_hours = self.spawn_intervals.get((rarity, size))\n",
        "        if interval_hours:\n",
        "            adjusted_time = last_drop_time + timedelta(hours=interval_hours)\n",
        "            next_spawn_time = adjusted_time.astimezone(pytz.timezone(\"Asia/Shanghai\")).strftime('%Y-%m-%d %H:%M')\n",
        "            return next_spawn_time\n",
        "        else:\n",
        "            return \"Invalid rarity or size\"\n",
        "\n",
        "    def save_to_csv(self, filename):\n",
        "        sorted_items = sorted(self.aggregated_data.items(), key=lambda x: datetime.strptime(x[0][2], '%Y-%m-%d %H:%M'))\n",
        "        with open(filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['稀有度', '大小', '下次掉落时间', '数量', '土地id'])\n",
        "            for (rarity, size, next_spawn_str), info in sorted_items:\n",
        "                writer.writerow([rarity, size, next_spawn_str, info['count'], ';'.join(map(str, info['issuedIds']))])\n",
        "\n",
        "    def run(self):\n",
        "        data = self.fetch_data()\n",
        "        if data:\n",
        "            self.process_data(data)\n",
        "            self.save_to_csv('combined_data.csv')\n",
        "            print(f\"数据已经保存到CSV文件：combined_data.csv\")\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "url = \"https://api.openloot.com/v2/market/items/in-game?gameId=56a149cf-f146-487a-8a1c-58dc9ff3a15c&page=1&pageSize=100&sort=name%3Aasc&tags=space\"\n",
        "\n",
        "cookies = {\n",
        "    'name': 'your_cookie_here'  # 请替换为实际的cookie值\n",
        "}\n",
        "\n",
        "aggregator = DataAggregator(url, cookies)\n",
        "aggregator.run()\n"
      ]
    }
  ]
}
